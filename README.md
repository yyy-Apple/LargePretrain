# LargePretrain

Pre-train large models. This repo is mostly for documenting my learning process of [DeepSpeed](https://github.com/microsoft/DeepSpeed)

## Relevant Paper Reading
### Megatron LM
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf)
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053v4.pdf)

### DeepSpeed
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
- [DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters]()
- [Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping]()
- [ZeRO-Offload: Democratizing Billion-Scale Model Training]()
- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning]()
- [1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed]()
- [Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training]()

### Others
> Referemces: <br>
> https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29<br>
> https://arxiv.org/abs/1802.09941

