# LargePretrain

Pre-train large models. This repo is mostly for documenting my learning process of [DeepSpeed](https://github.com/microsoft/DeepSpeed)

## Relevant Paper Reading
### Megatron LM
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf)
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053v4.pdf)

### DeepSpeed

> Referemces: <br>
> https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29<br>
> https://arxiv.org/abs/1802.09941

